{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch(1/1000) start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/61 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspace/research/BiDKT/src/visualizer.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f636f64696e676368696c64227d7d/workspace/research/BiDKT/src/visualizer.ipynb#ch0000001vscode-remote?line=0'>1</a>\u001b[0m train_scores, valid_scores, \\\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f636f64696e676368696c64227d7d/workspace/research/BiDKT/src/visualizer.ipynb#ch0000001vscode-remote?line=1'>2</a>\u001b[0m             best_valid_score, test_score, \\\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f7079746f7263685f646576222c2273657474696e6773223a7b22686f7374223a227373683a2f2f636f64696e676368696c64227d7d/workspace/research/BiDKT/src/visualizer.ipynb#ch0000001vscode-remote?line=2'>3</a>\u001b[0m                 attn_scores \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(train_loader, valid_loader, test_loader)\n",
      "File \u001b[0;32m/workspace/research/BiDKT/src/trainers/monaconvbert4kt_plus_diff_trainer_inference.py:331\u001b[0m, in \u001b[0;36mMonaConvBert4ktPlusDiffTrainerInference.train\u001b[0;34m(self, train_loader, valid_loader, test_loader, config)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch(\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m) start\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[1;32m    326\u001b[0m     epoch_index \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[1;32m    327\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epochs\n\u001b[1;32m    328\u001b[0m ))\n\u001b[1;32m    330\u001b[0m \u001b[39m# Training Session\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m train_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(train_loader, metric_name)\n\u001b[1;32m    332\u001b[0m valid_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate(valid_loader, metric_name)\n\u001b[1;32m    334\u001b[0m \u001b[39m# train, test record 저장\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/research/BiDKT/src/trainers/monaconvbert4kt_plus_diff_trainer_inference.py:147\u001b[0m, in \u001b[0;36mMonaConvBert4ktPlusDiffTrainerInference._train\u001b[0;34m(self, train_loader, metric_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m mlm_r_seqs \u001b[39m=\u001b[39m mlm_r_seqs\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    145\u001b[0m mlm_idxs \u001b[39m=\u001b[39m mlm_idxs\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 147\u001b[0m y_hat, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    148\u001b[0m     q_seqs\u001b[39m.\u001b[39;49mlong(), \n\u001b[1;32m    149\u001b[0m     mlm_r_seqs\u001b[39m.\u001b[39;49mlong(), \u001b[39m# r_seqs with MLM\u001b[39;49;00m\n\u001b[1;32m    150\u001b[0m     pid_seqs\u001b[39m.\u001b[39;49mlong(),\n\u001b[1;32m    151\u001b[0m     diff_seqs\u001b[39m.\u001b[39;49mlong(),\n\u001b[1;32m    152\u001b[0m     mask_seqs\u001b[39m.\u001b[39;49mlong() \u001b[39m# for attn_mask\u001b[39;49;00m\n\u001b[1;32m    153\u001b[0m )\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    154\u001b[0m \u001b[39m# |y_hat| = (bs, n, output_size=1)\u001b[39;00m\n\u001b[1;32m    156\u001b[0m y_hat \u001b[39m=\u001b[39m y_hat\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/research/BiDKT/src/models/monaconvbert4kt_plus_diff_inference.py:459\u001b[0m, in \u001b[0;36mMonaConvBert4ktPlusDiff.forward\u001b[0;34m(self, q, r, pid, diff, mask)\u001b[0m\n\u001b[1;32m    454\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_dropout(emb)\n\u001b[1;32m    455\u001b[0m \u001b[39m# |z| = (bs, n, emb_size)\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \n\u001b[1;32m    457\u001b[0m \u001b[39m# |mask_enc| = (bs, n, n)\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[39m# |z| = (bs, n, emb_size)\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m z, _, attn_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(z, mask)\n\u001b[1;32m    460\u001b[0m \u001b[39m# |z| = (bs, n, hs)\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \n\u001b[1;32m    462\u001b[0m \u001b[39m# get attention weigth\u001b[39;00m\n\u001b[1;32m    464\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator(z)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/research/BiDKT/src/models/monaconvbert4kt_plus_diff_inference.py:365\u001b[0m, in \u001b[0;36mMySequential.forward\u001b[0;34m(self, *x)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mx):\n\u001b[1;32m    360\u001b[0m     \u001b[39m# nn.Sequential class does not provide multiple input arguments and returns.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     \u001b[39m# Thus, we need to define new class to solve this issue.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[39m# Note that each block has same function interface.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m--> 365\u001b[0m         x \u001b[39m=\u001b[39m module(\u001b[39m*\u001b[39;49mx)\n\u001b[1;32m    367\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
